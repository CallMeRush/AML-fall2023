{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- try different wavelets and classifier parameters\n",
    "- stacking classifier?\n",
    "- sample from classes such that they all have same cardinality\n",
    "- try with CNN (or RNN)\n",
    "- hiearchical classification: first classify 3 vs all, then 0 vs 1 vs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_extraction import *\n",
    "import scipy\n",
    "from scipy import fft\n",
    "from scipy import signal\n",
    "from collections import Counter\n",
    "import pywt\n",
    "from biosppy.signals import ecg\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 1\n",
    "random_state = 71\n",
    "sampling_rate = 300\n",
    "data_directory = 'data/'\n",
    "\n",
    "waveletname = 'db31'\n",
    "level = 5 #10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.read_csv(data_directory + 'X_train.csv', index_col='id')\n",
    "y_train_df = pd.read_csv(data_directory + 'y_train.csv', index_col='id')\n",
    "X_test_df = pd.read_csv(data_directory + 'X_test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop trailing NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_trailing_na(df: pd.DataFrame):\n",
    "    return [df.loc[i].dropna().to_numpy() for i in range(df.shape[0])]\n",
    "\n",
    "X_train_full = drop_trailing_na(X_train_df)\n",
    "y_train_full = y_train_df['y'].to_numpy()\n",
    "X_test = drop_trailing_na(X_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train_full_filtered = wavelet_noise_cancellation_bulk(X_train_full)\\nX_test_filtered = wavelet_noise_cancellation_bulk(X_test)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wavelet_transform(signal):\n",
    "    return pywt.wavedec(signal, waveletname, level=level)\n",
    "\n",
    "def wavelet_noise_cancellation(signal):\n",
    "    coeffs = wavelet_transform(signal)\n",
    "    return pywt.waverec(coeffs, waveletname)\n",
    "\n",
    "def wavelet_noise_cancellation_bulk(data):\n",
    "    result = []\n",
    "    for signal in data:\n",
    "        result.append(wavelet_noise_cancellation(signal))\n",
    "    return result\n",
    "\n",
    "\"\"\"X_train_full_filtered = wavelet_noise_cancellation_bulk(X_train_full)\n",
    "X_test_filtered = wavelet_noise_cancellation_bulk(X_test)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(list_values):\n",
    "    value, probabilities = np.unique(list_values, return_counts=True)\n",
    "    entropy = scipy.stats.entropy(probabilities)\n",
    "    return [entropy]\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    " \n",
    "def calculate_statistics(list_values):\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values**2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    "\n",
    "def get_array_features(arr):\n",
    "    features = []\n",
    "    features += calculate_entropy(arr)\n",
    "    features += calculate_crossings(arr)\n",
    "    features += calculate_statistics(arr)\n",
    "    return features\n",
    "\n",
    "def get_wavelet_features(signal):\n",
    "    features = []\n",
    "    list_coeff = wavelet_transform(signal)\n",
    "    for coeff in list_coeff:\n",
    "        features += get_array_features(coeff)\n",
    "    return features\n",
    "\n",
    "def calculate_consecutive_diff(x):\n",
    "    return np.ediff1d(x)\n",
    "\n",
    "def get_values(template, r_peaks, peaks):\n",
    "    result = []\n",
    "    for i in range(len(peaks)):\n",
    "        result.append(template[i][peaks[i] - r_peaks[i] + 60])\n",
    "    return np.array(result)\n",
    "\n",
    "def get_ecg_values(signal):\n",
    "    result = ecg.ecg(signal, sampling_rate=sampling_rate, show=False)\n",
    "    template = result['templates']\n",
    "\n",
    "    p_peaks, p_start, p_end = getPPositions(result)\n",
    "\n",
    "    q_peaks, q_start = getQPositions(result)\n",
    "    for i in range(len(q_start)):\n",
    "        if q_start[i] == p_peaks[i]:\n",
    "            q_start[i] = int(p_end[i] + abs(q_peaks[i] - p_end[i]) / 2)\n",
    "\n",
    "    r_peaks = result['rpeaks'].tolist()\n",
    "\n",
    "    s_peaks, s_end = getSPositions(result)\n",
    "    \n",
    "    t_peaks, t_start, t_end = getTPositions(result)\n",
    "    \n",
    "    beats = fft.fft(template)\n",
    "    heart_rate = sampling_rate * (60.0 / np.diff(result['rpeaks']))\n",
    "    heart_rate = np.append(heart_rate, heart_rate[-1]).reshape(-1, 1)\n",
    "\n",
    "    # They are of length = # heart beats - 1 !!!\n",
    "    RRinterval = calculate_consecutive_diff(r_peaks)\n",
    "    PPinterval = calculate_consecutive_diff(p_peaks)\n",
    "    TPinterval = p_start[1:] - t_end[:-1]\n",
    "\n",
    "    Pduration = p_end - p_start\n",
    "    PRsegment = q_start - p_end\n",
    "    PRinterval = q_start - p_start\n",
    "    QRScomplex = s_end - q_start\n",
    "    QTinterval = t_end - q_start\n",
    "    STsegment = t_start - s_end\n",
    "    STTsegment = t_end - s_end\n",
    "\n",
    "    p_values = get_values(template, r_peaks, p_peaks)\n",
    "    q_values = get_values(template, r_peaks, q_peaks)\n",
    "    r_values = get_values(template, r_peaks, r_peaks)\n",
    "    s_values = get_values(template, r_peaks, s_peaks)\n",
    "    t_values = get_values(template, r_peaks, t_peaks)\n",
    "\n",
    "    PQ_diff = q_peaks - p_peaks\n",
    "    PR_diff = r_peaks - p_peaks\n",
    "    PS_diff = s_peaks - p_peaks\n",
    "    PT_diff = t_peaks - p_peaks\n",
    "    QR_diff = r_peaks - q_peaks\n",
    "    QS_diff = s_peaks - q_peaks\n",
    "    QT_diff = t_peaks - q_peaks\n",
    "    RS_diff = s_peaks - r_peaks\n",
    "    RT_diff = t_peaks - r_peaks\n",
    "    ST_diff = t_peaks - s_peaks\n",
    "\n",
    "    return heart_rate, np.real(beats), np.imag(beats), \\\n",
    "        RRinterval, PPinterval, Pduration, PRsegment, PRinterval, QRScomplex, QTinterval, STsegment, STTsegment, TPinterval, \\\n",
    "        p_values, q_values, r_values, s_values, t_values, \\\n",
    "        PQ_diff, PR_diff, PS_diff, PT_diff, QR_diff, QS_diff, QT_diff, RS_diff, RT_diff, ST_diff\n",
    "    ## Useful values if you want to use time-series-like arrays\n",
    "    # return p_start, p_peaks, p_end, q_start, q_peaks, r_peaks, s_peaks, s_end, t_start, t_peaks, t_end\n",
    "\n",
    "def get_features(signal):\n",
    "    features = []\n",
    "\n",
    "    ecg_values_list = get_ecg_values(signal)\n",
    "    for ecg_values in ecg_values_list:\n",
    "        features += get_array_features(ecg_values)\n",
    "\n",
    "    features += get_wavelet_features(signal)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_dataset_features(data):\n",
    "    list_features = []\n",
    "    for signal in data:\n",
    "        list_features.append(get_features(signal))\n",
    "    return list_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extracted = get_dataset_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_extracted = get_dataset_features(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.7998046875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "selection = SelectKBest(mutual_info_classif, k=260).fit(X_train_extracted, y_train.T.ravel())\n",
    "X_train_extracted2 = selection.transform(X_train_extracted)\n",
    "X_val_extracted2 = selection.transform(X_val_extracted)\n",
    "cls = XGBClassifier(seed=random_state)\n",
    "cls.fit(X_train_extracted2, y_train.T.ravel())\n",
    "\n",
    "y_train_pred = cls.predict(X_train_extracted2)\n",
    "y_val_pred = cls.predict(X_val_extracted2)\n",
    "train_score = f1_score(y_train, y_train_pred, average='micro')\n",
    "val_score = f1_score(y_val, y_val_pred, average='micro')\n",
    "\n",
    "print(train_score, val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls = GradientBoostingClassifier(n_estimators=100, verbose=1, random_state=random_state)\n",
    "cls = XGBClassifier(seed=random_state) #(n_estimators=100, gamma=1, reg_alpha=3, reg_lambda=0, max_depth=10, min_child_weight=0, colsample_bytree=0.85, seed=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier fit and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.fit(X_train_extracted, y_train.T.ravel())\n",
    "\n",
    "y_train_pred = cls.predict(X_train_extracted)\n",
    "y_val_pred = cls.predict(X_val_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.8046875\n"
     ]
    }
   ],
   "source": [
    "train_score = f1_score(y_train, y_train_pred, average='micro')\n",
    "val_score = f1_score(y_val, y_val_pred, average='micro')\n",
    "\n",
    "print(train_score, val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full_extracted = get_dataset_features(X_train_full)\n",
    "X_test_extracted = get_dataset_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(random_state=random_state)\n",
    "svc_model = Pipeline([ ('scaler', StandardScaler()), ('svc', svc) ])\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(random_state=random_state)\n",
    "etc_model = Pipeline([ ('scaler', StandardScaler()), ('etc', etc) ])\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bc = BaggingClassifier(estimator=svc_model, random_state=random_state)\n",
    "bc_model = Pipeline([ ('scaler', StandardScaler()), ('bc', bc) ])\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(estimator=svc_model, random_state=random_state)\n",
    "abc_model = Pipeline([ ('scaler', StandardScaler()), ('abc', abc) ])\n",
    "\n",
    "estimators = [\n",
    "    ('xgbc', cls),\n",
    "    ('svc', svc_model),\n",
    "    #('gb', gb_model),\n",
    "    ('etc', etc_model),\n",
    "    #('gpr', gpr_model),\n",
    "    ('bc', bc_model),\n",
    "    ('abc', abc_model)\n",
    "]\n",
    "\n",
    "final_pipeline = Pipeline([ ('model', LogisticRegression()) ])\n",
    "classifier = StackingClassifier(estimators, final_pipeline, n_jobs=n_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alrusso/AML-fall2023/env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = classifier.fit(X_train_full_extracted, y_train_full.T.ravel()).predict(X_test_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just XGBClassifier\n",
    "# y_test_pred = cls.fit(X_train_full_extracted, y_train_full.T.ravel()).predict(X_test_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame({'id': np.arange(0, y_test_pred.shape[0]), 'y': y_test_pred.flatten()})\n",
    "table.to_csv(data_directory + 'y_test_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
