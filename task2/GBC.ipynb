{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- try different wavelets and classifier parameters\n",
    "- stacking classifier?\n",
    "- sample from classes such that they all have same cardinality\n",
    "- try with CNN (or RNN)\n",
    "- hiearchical classification: first classify 3 vs all, then 0 vs 1 vs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_extraction import *\n",
    "import scipy\n",
    "from scipy import fft\n",
    "from scipy import signal\n",
    "from collections import Counter\n",
    "import pywt\n",
    "from biosppy.signals import ecg\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 1\n",
    "random_state = 71\n",
    "sampling_rate = 300\n",
    "data_directory = 'data/'\n",
    "\n",
    "waveletname = 'db31'\n",
    "level = 5 #10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.read_csv(data_directory + 'X_train.csv', index_col='id')\n",
    "y_train_df = pd.read_csv(data_directory + 'y_train.csv', index_col='id')\n",
    "X_test_df = pd.read_csv(data_directory + 'X_test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop trailing NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_trailing_na(df: pd.DataFrame):\n",
    "    return [df.loc[i].dropna().to_numpy() for i in range(df.shape[0])]\n",
    "\n",
    "X_train_full = drop_trailing_na(X_train_df)\n",
    "y_train_full = y_train_df['y'].to_numpy()\n",
    "X_test = drop_trailing_na(X_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_transform(signal):\n",
    "    return pywt.wavedec(signal, waveletname, level=level)\n",
    "\n",
    "def wavelet_noise_cancellation(signal):\n",
    "    coeffs = wavelet_transform(signal)\n",
    "    return pywt.waverec(coeffs, waveletname)\n",
    "\n",
    "def wavelet_noise_cancellation_bulk(data):\n",
    "    result = []\n",
    "    for signal in data:\n",
    "        result.append(wavelet_noise_cancellation(signal))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train_full_filtered = wavelet_noise_cancellation_bulk(X_train_full)\\nX_test_filtered = wavelet_noise_cancellation_bulk(X_test)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X_train_full_filtered = wavelet_noise_cancellation_bulk(X_train_full)\n",
    "X_test_filtered = wavelet_noise_cancellation_bulk(X_test)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(list_values):\n",
    "    value, probabilities = np.unique(list_values, return_counts=True)\n",
    "    \"\"\"counter_values = Counter(list_values).most_common()\n",
    "    probabilities = [elem[1]/len(list_values) for elem in counter_values]\"\"\"\n",
    "    entropy = scipy.stats.entropy(probabilities)\n",
    "    return [entropy]\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    " \n",
    "def calculate_statistics(list_values):\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values**2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    "\n",
    "def get_array_features(arr):\n",
    "    features = []\n",
    "    features += calculate_entropy(arr)\n",
    "    features += calculate_crossings(arr)\n",
    "    features += calculate_statistics(arr)\n",
    "    return features\n",
    "\n",
    "def get_wavelet_features(signal):\n",
    "    features = []\n",
    "    list_coeff = wavelet_transform(signal)\n",
    "    for coeff in list_coeff:\n",
    "        features += get_array_features(coeff)\n",
    "    return features\n",
    "\n",
    "def to_array(x):\n",
    "    return np.argwhere(x == 1).T[0]\n",
    "\n",
    "def calculate_consecutive_diff(x):\n",
    "    return np.ediff1d(x)\n",
    "\n",
    "def calculate_diff_old(x, y):\n",
    "    res = []\n",
    "    x_index, y_index = 0, 0\n",
    "    while x_index < len(x) and y_index < len(y):\n",
    "        if x_index < len(x) - 1 and x[x_index + 1] < y[y_index]:\n",
    "            x_index += 1\n",
    "        elif x[x_index] > y[y_index]:\n",
    "            y_index += 1\n",
    "        else:\n",
    "            res.append(y[y_index] - x[x_index])\n",
    "            x_index += 1\n",
    "            y_index += 1\n",
    "    return np.array(res)\n",
    "\n",
    "def calculate_diff(x, y):\n",
    "    res = []\n",
    "    for i in range(len(x)):\n",
    "        res.append(y[i] - x[i])\n",
    "    return np.array(res)\n",
    "\n",
    "def get_ecg_values_old(data):\n",
    "    result = ecg.ecg(data, sampling_rate=sampling_rate, show=False)\n",
    "\n",
    "    templates = result['templates']\n",
    "\n",
    "    p_peaks, q_peaks, r_peaks, s_peaks, t_peaks = [], [], [], [], []\n",
    "    p_values, q_values, r_values, s_values, t_values = [], [], [], [], []\n",
    "    pr_diff, ps_diff, pt_diff, qs_diff, qt_diff, rt_diff = [], [], [], [], [], []\n",
    "    # extract the pqrst indexes for each template (note that the indexes are sampled at 300Hz!)\n",
    "    for template in templates:\n",
    "        # calculate the locations\n",
    "        try:\n",
    "            # get local maximas and minimas with signal library\n",
    "            loc_max = np.array(signal.argrelextrema(template, np.greater))\n",
    "            loc_min = np.array(signal.argrelextrema(template, np.less))\n",
    "\n",
    "            # find the maximum, but cut the search area to the first half to avoid\n",
    "            # finding peaks at the wrong place\n",
    "            r = np.argmax(template[: int(len(template) / 2)])\n",
    "            # q and s are the first minima after and before the r value\n",
    "            q = loc_min[loc_min < r][-1]\n",
    "            s = loc_min[loc_min > r][0]\n",
    "            # p and t are the first maxima after and before the r value\n",
    "            p = loc_max[loc_max < r][-1]\n",
    "            t = loc_max[loc_max > r][0]\n",
    "\n",
    "            p_peaks.append(p)\n",
    "            p_values.append(template[p])\n",
    "            q_peaks.append(q)\n",
    "            q_values.append(template[q])\n",
    "            r_peaks.append(r)\n",
    "            r_values.append(template[r])\n",
    "            s_peaks.append(s)\n",
    "            s_values.append(template[s])\n",
    "            t_peaks.append(t)\n",
    "            t_values.append(template[t])\n",
    "            # calculate different values\n",
    "            pr_diff.append(r - p)\n",
    "            ps_diff.append(s - p)\n",
    "            pt_diff.append(t - p)\n",
    "            qs_diff.append(s - q)\n",
    "            qt_diff.append(t - q)\n",
    "            rt_diff.append(t - r)\n",
    "        except:\n",
    "            pr_diff.append(0)\n",
    "            ps_diff.append(0)\n",
    "            pt_diff.append(0)\n",
    "            qs_diff.append(0)\n",
    "            qt_diff.append(0)\n",
    "            rt_diff.append(0)\n",
    "\n",
    "    beats = fft.fft(templates)\n",
    "    heart_rate = sampling_rate * (60.0 / np.diff(result['rpeaks']))\n",
    "    heart_rate = np.append(heart_rate, heart_rate[-1]).reshape(-1, 1)\n",
    "    \n",
    "    \"\"\"RRinterval = calculate_consecutive_diff(r_peaks)\n",
    "    PPinterval = calculate_consecutive_diff(p_peaks)\"\"\"\n",
    "\n",
    "    \"\"\"Pduration = calculate_diff(ponsets, poffsets)\n",
    "    PRsegment = calculate_diff(poffsets, q_peaks)\n",
    "    PRinterval = calculate_diff(ponsets, q_peaks)\n",
    "    PRsegment = calculate_diff(poffsets, q_peaks)\n",
    "    QRScomplex = calculate_diff(q_peaks, s_peaks)\n",
    "    QTinterval = calculate_diff(q_peaks, toffsets)\n",
    "    STsegment = calculate_diff(s_peaks, tonsets)\n",
    "    STTsegment = calculate_diff(s_peaks, toffsets)\n",
    "    TPinterval = calculate_diff(toffsets, ponsets)\"\"\"\n",
    "\n",
    "    #return heart_rate, RRinterval, PPinterval, np.real(beats), np.imag(beats), \\\n",
    "    return heart_rate, np.real(beats), np.imag(beats), \\\n",
    "        np.array(p_values), np.array(q_values), np.array(r_values), np.array(s_values), np.array(t_values), \\\n",
    "        np.array(pr_diff), np.array(ps_diff), np.array(pt_diff), np.array(qs_diff), np.array(qt_diff), np.array(rt_diff)\n",
    "\n",
    "    \"\"\"p_vals.append(template[p])\n",
    "    q_vals.append(template[q])\n",
    "    r_vals.append(template[r])\n",
    "    s_vals.append(template[s])\n",
    "    t_vals.append(template[t])\"\"\"\n",
    "\n",
    "    \"\"\"rpeaks_diff = np.ediff1d(rpeaks)\n",
    "    return rpeaks_diff, templates\"\"\"\n",
    "    \n",
    "    signals, rpeaks = nk.ecg_process(data, sampling_rate=sampling_rate)\n",
    "\n",
    "    heart_rate = np.array(signals['ECG_Rate'])\n",
    "\n",
    "    ppeaks = to_array(signals['ECG_P_Peaks'])\n",
    "    ponsets = to_array(signals['ECG_P_Onsets'])\n",
    "    poffsets = to_array(signals['ECG_P_Offsets'])\n",
    "    qpeaks = to_array(signals['ECG_Q_Peaks'])\n",
    "    rpeaks = to_array(signals['ECG_R_Peaks'])\n",
    "    ronsets = to_array(signals['ECG_R_Onsets'])\n",
    "    roffsets = to_array(signals['ECG_R_Offsets'])\n",
    "    speaks = to_array(signals['ECG_S_Peaks'])\n",
    "    tpeaks = to_array(signals['ECG_T_Peaks'])\n",
    "    tonsets = to_array(signals['ECG_T_Onsets'])\n",
    "    toffsets = to_array(signals['ECG_T_Offsets'])\n",
    "\n",
    "    RRinterval = calculate_consecutive_diff(rpeaks)\n",
    "    PPinterval = calculate_consecutive_diff(ppeaks)\n",
    "\n",
    "    Pduration = calculate_diff(ponsets, poffsets)\n",
    "    PRsegment = calculate_diff(poffsets, qpeaks)\n",
    "    PRinterval = calculate_diff(ponsets, qpeaks)\n",
    "    PRsegment = calculate_diff(poffsets, qpeaks)\n",
    "    QRScomplex = calculate_diff(qpeaks, speaks)\n",
    "    QTinterval = calculate_diff(qpeaks, toffsets)\n",
    "    STsegment = calculate_diff(speaks, tonsets)\n",
    "    STTsegment = calculate_diff(speaks, toffsets)\n",
    "    TPinterval = calculate_diff(toffsets, ponsets)\n",
    "    \n",
    "    return heart_rate, RRinterval, PPinterval, Pduration, PRsegment, PRinterval, \\\n",
    "        PRsegment, QRScomplex, QTinterval, STsegment, STTsegment, TPinterval\n",
    "\n",
    "def get_values(template, r_peaks, peaks):\n",
    "    result = []\n",
    "    for i in range(len(peaks)):\n",
    "        result.append(template[i][peaks[i] - r_peaks[i] + 60])\n",
    "    return np.array(result)\n",
    "\n",
    "def get_ecg_values(signal):\n",
    "    result = ecg.ecg(signal, sampling_rate=sampling_rate, show=False)\n",
    "    template = result['templates']\n",
    "\n",
    "    p_peaks, p_start, p_end = getPPositions(result)\n",
    "\n",
    "    q_peaks, q_start = getQPositions(result)\n",
    "    for i in range(len(q_start)):\n",
    "        if q_start[i] == p_peaks[i]:\n",
    "            q_start[i] = int(p_end[i] + abs(q_peaks[i] - p_end[i]) / 2)\n",
    "\n",
    "    r_peaks = result['rpeaks'].tolist()\n",
    "\n",
    "    s_peaks, s_end = getSPositions(result)\n",
    "    \n",
    "    t_peaks, t_start, t_end = getTPositions(result)\n",
    "    \n",
    "    beats = fft.fft(template)\n",
    "    heart_rate = sampling_rate * (60.0 / np.diff(result['rpeaks']))\n",
    "    heart_rate = np.append(heart_rate, heart_rate[-1]).reshape(-1, 1)\n",
    "\n",
    "    # They are of length = # heart beats - 1 !!!\n",
    "    RRinterval = calculate_consecutive_diff(r_peaks)\n",
    "    PPinterval = calculate_consecutive_diff(p_peaks)\n",
    "    TPinterval = calculate_diff(t_end[:-1], p_start[1:])\n",
    "\n",
    "    p_end = np.array(p_end)\n",
    "    q_start = np.array(q_start)\n",
    "\n",
    "    Pduration = p_end - p_start # calculate_diff(p_start, p_end)\n",
    "    PRsegment = q_start - p_end # calculate_diff(p_end, q_start)\n",
    "    PRinterval = q_start - p_start # calculate_diff(p_start, q_start)\n",
    "    QRScomplex = calculate_diff(q_start, s_end)\n",
    "    QTinterval = calculate_diff(q_start, t_end)\n",
    "    STsegment = calculate_diff(s_end, t_start)\n",
    "    STTsegment = calculate_diff(s_end, t_end)\n",
    "\n",
    "    p_values = get_values(template, r_peaks, p_peaks)\n",
    "    q_values = get_values(template, r_peaks, q_peaks)\n",
    "    r_values = get_values(template, r_peaks, r_peaks)\n",
    "    s_values = get_values(template, r_peaks, s_peaks)\n",
    "    t_values = get_values(template, r_peaks, t_peaks)\n",
    "\n",
    "    PQ_diff = calculate_diff(p_peaks, q_peaks)\n",
    "    PR_diff = calculate_diff(p_peaks, r_peaks)\n",
    "    PS_diff = calculate_diff(p_peaks, s_peaks)\n",
    "    PT_diff = calculate_diff(p_peaks, t_peaks)\n",
    "    QR_diff = calculate_diff(q_peaks, r_peaks)\n",
    "    QS_diff = calculate_diff(q_peaks, s_peaks)\n",
    "    QT_diff = calculate_diff(q_peaks, t_peaks)\n",
    "    RS_diff = calculate_diff(r_peaks, s_peaks)\n",
    "    RT_diff = calculate_diff(r_peaks, t_peaks)\n",
    "    ST_diff = calculate_diff(s_peaks, t_peaks)\n",
    "\n",
    "    return heart_rate, np.real(beats), np.imag(beats), \\\n",
    "        RRinterval, PPinterval, Pduration, PRsegment, PRinterval, QRScomplex, QTinterval, STsegment, STTsegment, TPinterval, \\\n",
    "        p_values, q_values, r_values, s_values, t_values, \\\n",
    "        PQ_diff, PR_diff, PS_diff, PT_diff, QR_diff, QS_diff, QT_diff, RS_diff, RT_diff, ST_diff\n",
    "    #return p_start, p_peaks, p_end, q_start, q_peaks, r_peaks, s_peaks, s_end, t_start, t_peaks, t_end\n",
    "\n",
    "def get_features(signal):\n",
    "    features = []\n",
    "\n",
    "    ecg_values_list = get_ecg_values(signal)\n",
    "    for ecg_values in ecg_values_list:\n",
    "        features += get_array_features(ecg_values)\n",
    "\n",
    "    features += get_wavelet_features(signal)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_dataset_features(data):\n",
    "    list_features = []\n",
    "    for signal in data:\n",
    "        list_features.append(get_features(signal))\n",
    "    return list_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alrusso/AML-fall2023/task2/GBC.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_train_extracted \u001b[39m=\u001b[39m get_dataset_features(X_train)\n",
      "\u001b[1;32m/Users/alrusso/AML-fall2023/task2/GBC.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=258'>259</a>\u001b[0m list_features \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=259'>260</a>\u001b[0m \u001b[39mfor\u001b[39;00m signal \u001b[39min\u001b[39;00m data:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=260'>261</a>\u001b[0m     list_features\u001b[39m.\u001b[39mappend(get_features(signal))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=261'>262</a>\u001b[0m \u001b[39mreturn\u001b[39;00m list_features\n",
      "\u001b[1;32m/Users/alrusso/AML-fall2023/task2/GBC.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=249'>250</a>\u001b[0m ecg_values_list \u001b[39m=\u001b[39m get_ecg_values(signal)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=250'>251</a>\u001b[0m \u001b[39mfor\u001b[39;00m ecg_values \u001b[39min\u001b[39;00m ecg_values_list:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m     features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m get_array_features(ecg_values)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=253'>254</a>\u001b[0m features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m get_wavelet_features(signal)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=255'>256</a>\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "\u001b[1;32m/Users/alrusso/AML-fall2023/task2/GBC.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_array_features\u001b[39m(arr):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     features \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m calculate_entropy(arr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m calculate_crossings(arr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m calculate_statistics(arr)\n",
      "\u001b[1;32m/Users/alrusso/AML-fall2023/task2/GBC.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m value, probabilities \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(list_values, return_counts\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"counter_values = Counter(list_values).most_common()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprobabilities = [elem[1]/len(list_values) for elem in counter_values]\"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m entropy \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49mstats\u001b[39m.\u001b[39;49mentropy(probabilities)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alrusso/AML-fall2023/task2/GBC.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [entropy]\n",
      "File \u001b[0;32m~/AML-fall2023/env/lib/python3.11/site-packages/scipy/stats/_entropy.py:135\u001b[0m, in \u001b[0;36mentropy\u001b[0;34m(pk, qk, base, axis)\u001b[0m\n\u001b[1;32m    133\u001b[0m pk \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m*\u001b[39mpk \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(pk, axis\u001b[39m=\u001b[39maxis, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m qk \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     vec \u001b[39m=\u001b[39m special\u001b[39m.\u001b[39;49mentr(pk)\n\u001b[1;32m    136\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     qk \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(qk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_extracted = get_dataset_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_extracted = get_dataset_features(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls = GradientBoostingClassifier(n_estimators=100, verbose=1, random_state=random_state)\n",
    "cls = XGBClassifier(seed=random_state) #(n_estimators=100, gamma=1, reg_alpha=3, reg_lambda=0, max_depth=10, min_child_weight=0, colsample_bytree=0.85, seed=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier fit and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.fit(X_train_extracted, y_train.T.ravel())\n",
    "\n",
    "y_train_pred = cls.predict(X_train_extracted)\n",
    "y_val_pred = cls.predict(X_val_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = f1_score(y_train, y_train_pred, average='micro')\n",
    "val_score = f1_score(y_val, y_val_pred, average='micro')\n",
    "\n",
    "print(train_score, val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full_extracted = get_dataset_features(X_train_full)\n",
    "X_test_extracted = get_dataset_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(random_state=random_state)\n",
    "svc_model = Pipeline([ ('scaler', StandardScaler()), ('svc', svc) ])\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(random_state=random_state)\n",
    "etc_model = Pipeline([ ('scaler', StandardScaler()), ('etc', etc) ])\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bc = BaggingClassifier(estimator=svc_model, random_state=random_state)\n",
    "bc_model = Pipeline([ ('scaler', StandardScaler()), ('bc', bc) ])\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(estimator=svc_model, random_state=random_state)\n",
    "abc_model = Pipeline([ ('scaler', StandardScaler()), ('abc', abc) ])\n",
    "\n",
    "estimators = [\n",
    "    ('xgbc', cls),\n",
    "    ('svc', svc_model),\n",
    "    #('gb', gb_model),\n",
    "    ('etc', etc_model),\n",
    "    #('gpr', gpr_model),\n",
    "    ('bc', bc_model),\n",
    "    ('abc', abc_model)\n",
    "]\n",
    "\n",
    "final_pipeline = Pipeline([ ('model', LogisticRegression()) ])\n",
    "classifier = StackingClassifier(estimators, final_pipeline, n_jobs=n_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alrusso/AML-fall2023/env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = classifier.fit(X_train_full_extracted, y_train_full.T.ravel()).predict(X_test_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just XGBClassifier\n",
    "# y_test_pred = cls.fit(X_train_full_extracted, y_train_full.T.ravel()).predict(X_test_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame({'id': np.arange(0, y_test_pred.shape[0]), 'y': y_test_pred.flatten()})\n",
    "table.to_csv(data_directory + 'y_test_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
