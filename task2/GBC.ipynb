{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- try different wavelets and classifier parameters\n",
    "- stacking classifier?\n",
    "- sample from classes such that they all have same cardinality\n",
    "- try with CNN (or RNN)\n",
    "- hiearchical classification: first classify 3 vs all, then 0 vs 1 vs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_extraction import *\n",
    "import scipy\n",
    "from scipy import fft\n",
    "from scipy import signal\n",
    "from collections import Counter\n",
    "import pywt\n",
    "from biosppy.signals import ecg\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 1\n",
    "random_state = 71\n",
    "sampling_rate = 300\n",
    "data_directory = 'data/'\n",
    "\n",
    "#class_weights = {0: 3030/5117, 1: 443/5117, 2: 1474/5117, 3: 170/5117}\n",
    "waveletname = 'db31'\n",
    "level = 5 #10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.read_csv(data_directory + 'X_train.csv', index_col='id')\n",
    "y_train_df = pd.read_csv(data_directory + 'y_train.csv', index_col='id')\n",
    "X_test_df = pd.read_csv(data_directory + 'X_test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop trailing NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_trailing_na(df: pd.DataFrame):\n",
    "    return [df.loc[i].dropna().to_numpy() for i in range(df.shape[0])]\n",
    "\n",
    "X_train_full = drop_trailing_na(X_train_df)\n",
    "y_train_full = y_train_df['y'].to_numpy()\n",
    "X_test = drop_trailing_na(X_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_transform(signal):\n",
    "    return pywt.wavedec(signal, waveletname, level=level)\n",
    "\n",
    "def wavelet_noise_cancellation(signal):\n",
    "    coeffs = wavelet_transform(signal)\n",
    "    return pywt.waverec(coeffs, waveletname)\n",
    "\n",
    "def wavelet_noise_cancellation_bulk(data):\n",
    "    result = []\n",
    "    for signal in data:\n",
    "        result.append(wavelet_noise_cancellation(signal))\n",
    "    return result\n",
    "\n",
    "\"\"\"X_train_full_filtered = wavelet_noise_cancellation_bulk(X_train_full)\n",
    "X_test_filtered = wavelet_noise_cancellation_bulk(X_test)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(list_values):\n",
    "    value, probabilities = np.unique(list_values, return_counts=True)\n",
    "    entropy = scipy.stats.entropy(probabilities)\n",
    "    return [entropy]\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    " \n",
    "def calculate_statistics(list_values):\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values**2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    "\n",
    "def get_array_features(arr):\n",
    "    features = []\n",
    "    features += calculate_entropy(arr)\n",
    "    features += calculate_crossings(arr)\n",
    "    features += calculate_statistics(arr)\n",
    "    return features\n",
    "\n",
    "def get_wavelet_features(signal):\n",
    "    features = []\n",
    "    list_coeff = wavelet_transform(signal)\n",
    "    for coeff in list_coeff:\n",
    "        features += get_array_features(coeff)\n",
    "    return features\n",
    "\n",
    "def calculate_consecutive_diff(x):\n",
    "    return np.ediff1d(x)\n",
    "\n",
    "def get_values(template, r_peaks, peaks):\n",
    "    result = []\n",
    "    for i in range(len(peaks)):\n",
    "        result.append(template[i][peaks[i] - r_peaks[i] + 60])\n",
    "    return np.array(result)\n",
    "\n",
    "def get_ecg_values(signal):\n",
    "    result = ecg.ecg(signal, sampling_rate=sampling_rate, show=False)\n",
    "    template = result['templates']\n",
    "\n",
    "    p_peaks, p_start, p_end = getPPositions(result)\n",
    "\n",
    "    q_peaks, q_start = getQPositions(result)\n",
    "    for i in range(len(q_start)):\n",
    "        if q_start[i] == p_peaks[i]:\n",
    "            q_start[i] = int(p_end[i] + abs(q_peaks[i] - p_end[i]) / 2)\n",
    "\n",
    "    r_peaks = result['rpeaks'].tolist()\n",
    "\n",
    "    s_peaks, s_end = getSPositions(result)\n",
    "    \n",
    "    t_peaks, t_start, t_end = getTPositions(result)\n",
    "    \n",
    "    beats = fft.fft(template)\n",
    "    heart_rate = sampling_rate * (60.0 / np.diff(result['rpeaks']))\n",
    "    heart_rate = np.append(heart_rate, heart_rate[-1]).reshape(-1, 1)\n",
    "\n",
    "    # They are of length = # heart beats - 1 !!!\n",
    "    RRinterval = calculate_consecutive_diff(r_peaks)\n",
    "    PPinterval = calculate_consecutive_diff(p_peaks)\n",
    "    TPinterval = p_start[1:] - t_end[:-1]\n",
    "\n",
    "    Pduration = p_end - p_start\n",
    "    PRsegment = q_start - p_end\n",
    "    PRinterval = q_start - p_start\n",
    "    QRScomplex = s_end - q_start\n",
    "    QTinterval = t_end - q_start\n",
    "    STsegment = t_start - s_end\n",
    "    STTsegment = t_end - s_end\n",
    "\n",
    "    p_values = get_values(template, r_peaks, p_peaks)\n",
    "    q_values = get_values(template, r_peaks, q_peaks)\n",
    "    r_values = get_values(template, r_peaks, r_peaks)\n",
    "    s_values = get_values(template, r_peaks, s_peaks)\n",
    "    t_values = get_values(template, r_peaks, t_peaks)\n",
    "\n",
    "    PQ_diff = q_peaks - p_peaks\n",
    "    PR_diff = r_peaks - p_peaks\n",
    "    PS_diff = s_peaks - p_peaks\n",
    "    PT_diff = t_peaks - p_peaks\n",
    "    QR_diff = r_peaks - q_peaks\n",
    "    QS_diff = s_peaks - q_peaks\n",
    "    QT_diff = t_peaks - q_peaks\n",
    "    RS_diff = s_peaks - r_peaks\n",
    "    RT_diff = t_peaks - r_peaks\n",
    "    ST_diff = t_peaks - s_peaks\n",
    "\n",
    "    return heart_rate, np.real(beats), np.imag(beats), \\\n",
    "        RRinterval, PPinterval, Pduration, PRsegment, PRinterval, QRScomplex, QTinterval, STsegment, STTsegment, TPinterval, \\\n",
    "        p_values, q_values, r_values, s_values, t_values, \\\n",
    "        PQ_diff, PR_diff, PS_diff, PT_diff, QR_diff, QS_diff, QT_diff, RS_diff, RT_diff, ST_diff\n",
    "    ## Useful values if you want to use time-series-like arrays\n",
    "    # return p_start, p_peaks, p_end, q_start, q_peaks, r_peaks, s_peaks, s_end, t_start, t_peaks, t_end\n",
    "\n",
    "def get_features(signal):\n",
    "    features = []\n",
    "\n",
    "    ecg_values_list = get_ecg_values(signal)\n",
    "    for ecg_values in ecg_values_list:\n",
    "        features += get_array_features(ecg_values)\n",
    "\n",
    "    features += get_wavelet_features(signal)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_dataset_features(data):\n",
    "    list_features = []\n",
    "    for signal in data:\n",
    "        list_features.append(get_features(signal))\n",
    "    return list_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full_extracted = get_dataset_features(X_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extracted, X_val_extracted, y_train, y_val = train_test_split(X_train_full_extracted, y_train_full, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Values that can still be tweaked:\n",
    "- etc: n_estimators\n",
    "- both bc and abc have not been tuned, so are not used at the moment\n",
    "\n",
    "Lastly: try one-vs-rest vs one-vs-one\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def get_cls012(X_train):\n",
    "    xgbc = XGBClassifier(seed=random_state)\n",
    "    return xgbc # This yields the best result, better than the below combination of different classifiers...\n",
    "\n",
    "    #xgbc_model = Pipeline([ ('scaler', StandardScaler()), ('xgbc', xgbc) ])\n",
    "    xgbc_model = Pipeline([ ('xgbc', xgbc) ])\n",
    "\n",
    "    svc = SVC(C=10, kernel='rbf', degree=1, gamma='auto', random_state=random_state)\n",
    "    svc_model = Pipeline([ ('scaler', StandardScaler()), ('svc', svc) ])\n",
    "\n",
    "    n_quantiles = int(len(X_train)*2/5 / 5)\n",
    "    gbc = GradientBoostingClassifier(n_estimators=500, min_samples_split=2, min_samples_leaf=1, learning_rate=0.7, max_depth=8, max_features=None, random_state=random_state)\n",
    "    scaler = QuantileTransformer(n_quantiles=int(n_quantiles/5), output_distribution=\"normal\")\n",
    "    gbc_model = Pipeline([ ('scaler', scaler), ('gbc', gbc) ])\n",
    "\n",
    "    etc = ExtraTreesClassifier(n_estimators=500, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=random_state)\n",
    "    etc_model = Pipeline([ ('scaler', StandardScaler()), ('etc', etc) ])\n",
    "\n",
    "    bc = BaggingClassifier(estimator=svc_model, random_state=random_state)\n",
    "    bc_model = Pipeline([ ('scaler', StandardScaler()), ('bc', bc) ])\n",
    "\n",
    "    abc = AdaBoostClassifier(estimator=svc_model, random_state=random_state)\n",
    "    abc_model = Pipeline([ ('scaler', StandardScaler()), ('abc', abc) ])\n",
    "\n",
    "    estimators = [\n",
    "        ('xgbc', xgbc_model),\n",
    "        ('svc', svc_model),\n",
    "        ('gbc', gbc_model),\n",
    "        ('etc', etc_model),\n",
    "        #('gpr', gpr_model),\n",
    "        #('bc', bc_model),\n",
    "        #('abc', abc_model)\n",
    "    ]\n",
    "\n",
    "    final_pipeline = Pipeline([ ('model', LogisticRegression()) ])\n",
    "    cls_012 = StackingClassifier(estimators, final_pipeline, n_jobs=n_cores)\n",
    "\n",
    "    return cls_012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_classification(X_train, y_train, X_test, y_test=[]):\n",
    "    y_train_01 = np.copy(y_train)\n",
    "    y_train_01[y_train <= 2] = 0\n",
    "    y_train_01[y_train == 3] = 1\n",
    "    X_train_01_oversampled, y_train_01_oversampled = SMOTE(random_state=random_state).fit_resample(X_train, y_train_01)\n",
    "\n",
    "    cls_01 = XGBClassifier(seed=random_state)\n",
    "    y_test_01_pred = cls_01.fit(X_train_01_oversampled, y_train_01_oversampled.T.ravel()).predict(X_test)\n",
    "    \n",
    "    if len(y_test) != 0:\n",
    "        y_test_01 = np.copy(y_test)\n",
    "        y_test_01[y_test <= 2] = 0\n",
    "        y_test_01[y_test == 3] = 1\n",
    "        print(\"Class 0/1/2 vs 3 score:\", f1_score(y_test_01, y_test_01_pred, average='micro'))\n",
    "    \n",
    "    X_train_012 = np.copy(X_train)[y_train <= 2]\n",
    "    y_train_012 = np.copy(y_train)[y_train <= 2]\n",
    "    X_train_012_oversampled, y_train_012_oversampled = SMOTE(random_state=random_state).fit_resample(X_train_012, y_train_012)\n",
    "\n",
    "    cls_012 = get_cls012(X_train_012_oversampled)\n",
    "\n",
    "    X_test_012 = np.copy(X_test)[y_test_01_pred == 0]\n",
    "    y_test_012_pred = cls_012.fit(X_train_012_oversampled, y_train_012_oversampled.T.ravel()).predict(X_test_012)\n",
    "    \n",
    "    y_test_pred = []\n",
    "    count_01, count_012 = 0, 0\n",
    "    for i in range(len(y_test_01_pred)):\n",
    "        if y_test_01_pred[i] == 0:\n",
    "            y_test_pred.append(y_test_012_pred[count_012])\n",
    "            count_012 += 1\n",
    "        else:\n",
    "            y_test_pred.append(3)\n",
    "            count_01 += 1\n",
    "    y_test_pred = np.array(y_test_pred)\n",
    "\n",
    "    if len(y_test) != 0:\n",
    "        y_test_012 = np.copy(y_test)[y_test_01_pred == 0]\n",
    "        print(\"Class 0 vs 1 vs 2 score:\", f1_score(y_test_012, y_test_012_pred, average='micro'))\n",
    "                \n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = hierarchical_classification(X_train_extracted, y_train, X_val_extracted, y_test=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_extracted = get_dataset_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full_extracted_oversampled, y_train_full_oversampled = SMOTE().fit_resample(X_train_full_extracted, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = hierarchical_classification(X_train_full_extracted, y_train_full, X_test_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame({'id': np.arange(0, y_test_pred.shape[0]), 'y': y_test_pred.flatten()})\n",
    "table.to_csv(data_directory + 'y_test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests and tuning\n",
    "\n",
    "Can be ignored for just a \"normal\"/best prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier hyperparameters tuning\n",
    "\n",
    "Can be ignored, in order to just train the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_01_2 = np.copy(y_train)\n",
    "y_train_01_2[y_train <= 2] = 0\n",
    "y_train_01_2[y_train == 3] = 1\n",
    "X_train_01_oversampled_2, y_train_01_oversampled_2 = SMOTE(random_state=random_state).fit_resample(X_train_extracted, y_train_01_2)\n",
    "y_val_01_2 = np.copy(y_val)\n",
    "y_val_01_2[y_val <= 2] = 0\n",
    "y_val_01_2[y_val == 3] = 1\n",
    "\n",
    "y_val_01_pred_2 = XGBClassifier(seed=random_state).fit(X_train_01_oversampled_2, y_train_01_oversampled_2.T.ravel()).predict(X_val_extracted)\n",
    "\n",
    "X_train_012_2 = np.copy(X_train_extracted)[y_train <= 2]\n",
    "y_train_012_2 = np.copy(y_train)[y_train <= 2]\n",
    "X_train_012_oversampled_2, y_train_012_oversampled_2 = SMOTE(random_state=random_state).fit_resample(X_train_012_2, y_train_012_2)\n",
    "\n",
    "X_val_extracted_012_2 = np.copy(X_val_extracted)[y_val_01_pred_2 == 0]\n",
    "y_val_012_2 = np.copy(y_val)[y_val_01_pred_2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def find_best_estimator(model, params, X_train, y_train):\n",
    "    estimator = GridSearchCV(model, params, scoring='f1_micro', cv=3, n_jobs=n_cores, verbose=3)\n",
    "    estimator.fit(X_train, y_train.T.ravel())\n",
    "    print(estimator.best_params_)\n",
    "    return estimator.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\"\"\"svc = SVC(random_state=random_state)\n",
    "svc_model = Pipeline([ ('scaler', StandardScaler()), ('svc', svc) ])\n",
    "params = [{\n",
    "    'svc__C': [10],\n",
    "    'svc__kernel': ['rbf'],#['rbf', 'poly', 'sigmoid', 'precomputed'], #'linear'\n",
    "    'svc__degree': [1],#, 2, 3, 4, 5],\n",
    "    'svc__gamma': ['auto']#, 'scale']\n",
    "}]\n",
    "model = Pipeline([ ('scaler', StandardScaler()), ('svc', svc) ])\"\"\"\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\"\"\"etr = ExtraTreesClassifier(random_state=random_state)\n",
    "params = [{\n",
    "    'etc__n_estimators': [500],\n",
    "    'etc__min_samples_split': [2],#, 3, 4, 5],\n",
    "    'etc__min_samples_leaf': [1],#, 2],\n",
    "    'etc__max_features': [None]\n",
    "}]\n",
    "model = Pipeline([ ('scaler', StandardScaler()), ('etc', etr) ])\"\"\"\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\"\"\"n_qantiles_full = int(len(X_train_012_oversampled_2)*2/5)\n",
    "gbc = GradientBoostingClassifier(random_state=random_state)\n",
    "scaler = QuantileTransformer(output_distribution=\"normal\")\n",
    "params = [{\n",
    "    'gbc__n_estimators': [100],\n",
    "    'gbc__min_samples_split': [2],\n",
    "    'gbc__min_samples_leaf': [1],\n",
    "    'gbc__learning_rate': [0.7],\n",
    "    'gbc__max_depth': [8],\n",
    "    'gbc__max_features': ['sqrt'],#, None],\n",
    "    'scaler__n_quantiles': [int(n_qantiles_full/5)]#, int(n_qantiles_full/4)]#, n_qantiles_full]\n",
    "}]\n",
    "model = Pipeline([ ('scaler', scaler), ('gbc', gbc) ])\"\"\"\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel, ConstantKernel, ExpSineSquared, RationalQuadratic\n",
    "k1 = ConstantKernel(constant_value=66.0**2) * RBF(length_scale=67.0)  # long term smooth rising trend\n",
    "k2 = ConstantKernel(constant_value=2.4**2) * RBF(length_scale=90.0) \\\n",
    "    * ExpSineSquared(length_scale=1.3, periodicity=1.0)  # seasonal component\n",
    "# medium term irregularity\n",
    "k3 = ConstantKernel(constant_value=0.66**2) \\\n",
    "    * RationalQuadratic(length_scale=1.2, alpha=0.78)\n",
    "k4 = ConstantKernel(constant_value=0.18**2) * RBF(length_scale=0.134) \\\n",
    "    + WhiteKernel(noise_level=0.19**2)  # noise terms\n",
    "kernel_gpml = k1 + k2 + k3 + k4\n",
    "gpc = GaussianProcessClassifier(kernel=kernel_gpml, random_state=random_state)\n",
    "params = [{\n",
    "    #'gpc__alpha': [0.3, 0.4],#np.logspace(-2, 4, 5),\n",
    "    'gpc__kernel__k1__k1__k1__k1__constant_value': [10000, 30000],\n",
    "    'gpc__kernel__k1__k1__k1__k2__length_scale': [0.5, 1, 2],\n",
    "    'gpc__kernel__k2__k2__noise_level': np.logspace(-2, 1, 5)\n",
    "}]\n",
    "model = Pipeline([ ('scaler', StandardScaler()), ('gpc', gpc) ])\n",
    "\"\"\"xgbc = XGBClassifier(seed=random_state)\n",
    "svc = SVC(C=10, kernel='rbf', degree=1, gamma='auto', random_state=random_state)\n",
    "svc_model = Pipeline([ ('scaler', StandardScaler()), ('svc', svc) ])\n",
    "n_quantiles = int(len(X_train_012_oversampled_2)*2/5 / 5)\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=1, learning_rate=0.7, max_depth=8, max_features=None, random_state=random_state)\n",
    "scaler = QuantileTransformer(n_quantiles=int(n_quantiles/5), output_distribution=\"normal\")\n",
    "gbc_model = Pipeline([ ('scaler', scaler), ('gbc', gbc) ])\n",
    "etc = ExtraTreesClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=random_state)\n",
    "etc_model = Pipeline([ ('scaler', StandardScaler()), ('etc', etc) ])\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bc = BaggingClassifier(random_state=random_state)\n",
    "params = [{\n",
    "    'bc__n_estimators': [10],#, 800],\n",
    "    'bc__estimator': [svc_model]#[xgbc, svc_model, gbc_model, etc_model]\n",
    "}]\n",
    "model = Pipeline([ ('scaler', StandardScaler()), ('bc', bc) ])\"\"\"\n",
    "\"\"\"xgbc = XGBClassifier(seed=random_state)\n",
    "params = [{\n",
    "    'xgbc__min_child_weight': [1],#, 5, 10],\n",
    "    'xgbc__gamma': [0.5],#, 1, 1.5, 2, 5],\n",
    "    'xgbc__subsample': [1.0],#[0.6, 0.8, 1.0],\n",
    "    'xgbc__colsample_bytree': [0.6],#, 0.8, 1.0],\n",
    "    'xgbc__max_depth': [3]#, 4, 5]\n",
    "}]\n",
    "model = Pipeline([ ('scaler', StandardScaler()), ('xgbc', XGBClassifier(seed=random_state)) ])\"\"\"\n",
    "\n",
    "cls = find_best_estimator(model, params, X_train_012_oversampled_2, y_train_012_oversampled_2)\n",
    "#cls = XGBClassifier(seed=random_state)\n",
    "#cls = svc_model\n",
    "cls.fit(X_train_012_oversampled_2, y_train_012_oversampled_2.T.ravel())\n",
    "\n",
    "y_train_pred = cls.predict(X_train_012_oversampled_2)\n",
    "y_val_pred = cls.predict(X_val_extracted_012_2)\n",
    "train_score = f1_score(y_train_012_oversampled_2, y_train_pred, average='micro')\n",
    "val_score = f1_score(y_val_012_2, y_val_pred, average='micro')\n",
    "\n",
    "print(train_score, val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting best features\n",
    "\n",
    "This yields a worse result than using all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "selection = SelectKBest(mutual_info_classif, k=260).fit(X_train_extracted, y_train.T.ravel())\n",
    "X_train_extracted2 = selection.transform(X_train_extracted)\n",
    "X_val_extracted2 = selection.transform(X_val_extracted)\n",
    "cls = XGBClassifier(seed=random_state)\n",
    "cls.fit(X_train_extracted2, y_train.T.ravel())\n",
    "\n",
    "y_train_pred = cls.predict(X_train_extracted2)\n",
    "y_val_pred = cls.predict(X_val_extracted2)\n",
    "train_score = f1_score(y_train, y_train_pred, average='micro')\n",
    "val_score = f1_score(y_val, y_val_pred, average='micro')\n",
    "\n",
    "print(train_score, val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification without hierarchical classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training/validation\n",
    "\n",
    "cls.fit(X_train_extracted, y_train.T.ravel())\n",
    "\n",
    "y_train_pred = cls.predict(X_train_extracted)\n",
    "y_val_pred = cls.predict(X_val_extracted)\n",
    "\n",
    "train_score = f1_score(y_train, y_train_pred, average='micro')\n",
    "val_score = f1_score(y_val, y_val_pred, average='micro')\n",
    "\n",
    "print(train_score, val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete classification without hierarchical classification\n",
    "y_test_pred = cls.fit(X_train_full_extracted, y_train_full.T.ravel()).predict(X_test_extracted)\n",
    "# y_test_pred = XGBClassifier(seed=random_state).fit(X_train_full_extracted, y_train_full.T.ravel()).predict(X_test_extracted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
